{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff89964c",
   "metadata": {},
   "source": [
    "# Notebook to use a model\n",
    "\n",
    "Once the model is trained and uploaded to the artifact server, we can use it in a new notebook. Here, we apply the model in a **distributed mode** with **PySpark**. For information, a version of pandas is available in PySpark and allows people unfamiliar with PySpark to benefit from the advantages of distributed.\n",
    "\n",
    "Since the datascientist does not have access to the production dataset, this notebook uses the test dataset and will be packaged to generate a punchline. The production data set will be used in this punchline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df1d97",
   "metadata": {},
   "source": [
    "### Adding dependencies to the environment\n",
    "\n",
    "We reuse the pex created in the previous notebook and add the model in the dependencies list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84abac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "++ java -Xmx1g -Xms256m -Dlog4j.configurationFile=/punch/conf/log4j2/log4j2-stdout.xml -cp /punch/resourcectl.jar com.github.punchplatform.resourcectl.ResourceCtl -u http://artifacts-server.punch-artifacts:4245 download -r additional-pex:demo:dependencies:1.0.0 -o /usr/share/punch/extlib/pyspark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource additional-pex:demo:dependencies:1.0.0 downloaded to /usr/share/punch/extlib/pyspark/dependencies-1.0.0.pex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "++ java -Xmx1g -Xms256m -Dlog4j.configurationFile=/punch/conf/log4j2/log4j2-stdout.xml -cp /punch/resourcectl.jar com.github.punchplatform.resourcectl.ResourceCtl -u http://artifacts-server.punch-artifacts:4245 download -r model:demo:credit_card:1.0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource model:demo:credit_card:1.0.0 downloaded to /usr/share/punch/artifacts/demo/credit_card/1.0.0/credit_card_1.0.0.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "Jupyter.notebook.session.restart({kernel_name: 'python3_punch_pyspark'})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%punch_dependencies\n",
    "additional-pex:demo:dependencies:1.0.0\n",
    "model:demo:credit_card:1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a459f818",
   "metadata": {},
   "source": [
    "### Changing number of executors\n",
    "\n",
    "You can change spark configuration with [punch_spark_session](https://punch-1.gitbook.io/punch-doc/v/welcome-to-the-punch/applications/jupyter/magic-commands#punchsparksession) to increase the number of executors for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ec3aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/14 11:03:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:SparkMonitorKernel:Client Connected ('127.0.0.1', 59054)\n"
     ]
    }
   ],
   "source": [
    "%%punch_spark_session -f\n",
    "{\n",
    "    \"spark.executor.instances\":3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f518a75",
   "metadata": {},
   "source": [
    "### Importing modules\n",
    "\n",
    "We chose to work with pyspark.pandas and our model is a mlflow package, so we will use the loading model function provided by them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cc57f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.pandas as pypd\n",
    "from pyspark.pandas.mlflow import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355c24e",
   "metadata": {},
   "source": [
    "### Loading the model\n",
    "\n",
    "Punch provides you a magic line to get back the path of the model into a variable. We can thus use this variable to load the model according to the model type (ex mlflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c49f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of files in the model directory:\n",
      "\t requirements.txt\n",
      "\t credit_card_1.0.0.zip\n",
      "\t conda.yaml\n",
      "\t MLmodel\n",
      "\t model.pkl\n",
      "\t python_env.yaml\n",
      "\n",
      "Model path is available in model_path variable.\n"
     ]
    }
   ],
   "source": [
    "%punch_get_model --model demo:credit_card:1.0.0 --output model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b14f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_model = load_model(model_uri=model_path, predict_type=\"double\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a292c",
   "metadata": {},
   "source": [
    "### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dae1279d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/14 11:04:15 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is available in data variable.\n",
      "Execution time: 0:00:04.233289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%punch_source --type file --name data -o \n",
    "options:\n",
    "    header: True\n",
    "path: s3a://demo/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb9b30b",
   "metadata": {},
   "source": [
    "### Converting PySpark SQL DataFrame into pyspark.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6cb9d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pypd.DataFrame(data)\n",
    "data = data[['distance_from_home', 'distance_from_last_transaction',\n",
    "       'ratio_to_median_purchase_price', 'repeat_retailer', 'used_chip',\n",
    "       'used_pin_number', 'online_order', 'fraud']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48b4eb5",
   "metadata": {},
   "source": [
    "### Adding parameters cell\n",
    "\n",
    "You can define parameters whose value can be overridden when the punchline is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42814989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "nb_rows = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "007c44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[0:nb_rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a336e4",
   "metadata": {},
   "source": [
    "### Application of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c06b0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/12/14 11:05:12 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>distance_from_home</th>\n",
       "      <th>distance_from_last_transaction</th>\n",
       "      <th>ratio_to_median_purchase_price</th>\n",
       "      <th>repeat_retailer</th>\n",
       "      <th>used_chip</th>\n",
       "      <th>used_pin_number</th>\n",
       "      <th>online_order</th>\n",
       "      <th>fraud</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.18884245347924</td>\n",
       "      <td>0.0677842994751078</td>\n",
       "      <td>1.659848080721224</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.359727748339491</td>\n",
       "      <td>0.1862579567074051</td>\n",
       "      <td>0.4952585147507252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.401608276239754</td>\n",
       "      <td>17.712807993684493</td>\n",
       "      <td>2.364811107092758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.102588133740203</td>\n",
       "      <td>0.2588216525296174</td>\n",
       "      <td>4.853085489890698</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.660351104672886</td>\n",
       "      <td>2.729079523776509</td>\n",
       "      <td>5.2572618573531855</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   distance_from_home distance_from_last_transaction ratio_to_median_purchase_price repeat_retailer used_chip used_pin_number online_order fraud  prediction\n",
       "0   11.18884245347924             0.0677842994751078              1.659848080721224             1.0       0.0             0.0          1.0   0.0         0.0\n",
       "1   8.359727748339491             0.1862579567074051             0.4952585147507252             1.0       1.0             0.0          0.0   0.0         0.0\n",
       "2  11.401608276239754             17.712807993684493              2.364811107092758             1.0       0.0             0.0          0.0   0.0         0.0\n",
       "3   3.102588133740203             0.2588216525296174              4.853085489890698             1.0       1.0             0.0          0.0   0.0         0.0\n",
       "4   4.660351104672886              2.729079523776509             5.2572618573531855             1.0       0.0             0.0          1.0   1.0         1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = data.drop('fraud', axis=1)\n",
    "prediction = credit_card_model.predict(features)\n",
    "features[\"prediction\"] = prediction\n",
    "columns = list(features.columns)\n",
    "columns.remove(\"prediction\")\n",
    "everything = data.merge(features, on=columns)\n",
    "everything.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b22df4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fraud  prediction\n",
       "0.0    0.0           9144\n",
       "1.0    1.0            855\n",
       "0.0    1.0              1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "everything.groupby([\"fraud\", \"prediction\"]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee15e09b",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61ed8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pex/installed_wheels/5a1df7cb4a7e7ce6147b717e58290c34aefce4a64408c8fe40c99f4e1f2c4a2b/pyspark-3.3.0-py2.py3-none-any.whl/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    }
   ],
   "source": [
    "everything = everything.to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c5cd9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/14 11:05:37 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved.\n",
      "Execution time: 0:00:13.247500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%punch_sink --type file -df everything\n",
    "options:\n",
    "    header: True\n",
    "format: csv\n",
    "path: s3a://demo/results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dff9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Punch Pyspark",
   "language": "python",
   "name": "python3_punch_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
